{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: LQR & ILQR\n",
    "\n",
    "In this exercise you will learn how to develop your own LQR and ILQR algorithms. For this task, we need to use CasADi: a library for symbolic calculation for python. This library composes functionalities not only symbolic calculation, but also automatic differentiation, numerical integration, linear and nonlinear programming solvers. We find various use cases for such tasks in control engineering, e.g., control for quadrotors and automotives. For further information and tutorials refer to the [CasADi documentation](https://web.casadi.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: need install **matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from ml_collections import config_dict\n",
    "from numpy.typing import NDArray\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from crazyflow.constants import GRAVITY, MASS, J\n",
    "from crazyflow.control import Control\n",
    "from crazyflow.control.control import MAX_THRUST, MIN_THRUST\n",
    "from crazyflow.sim.symbolic import symbolic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config for simulation\n",
    "sim_config = config_dict.ConfigDict()\n",
    "sim_config.control = Control.thrust\n",
    "sim_config.thrust_freq = 500\n",
    "sim_config.device = \"cpu\"\n",
    "sim_config.n_drones = 1\n",
    "sim_config.n_worlds = 1 #20\n",
    "\n",
    "envs = gymnasium.make_vec(\n",
    "    \"DroneReachPos-v0\",\n",
    "    num_envs=sim_config.n_worlds,\n",
    "    **sim_config,\n",
    ")\n",
    "\n",
    "print('observation space: \\n', envs.observation_space)\n",
    "print('action space: \\n', envs.action_space)\n",
    "print('time step:', 1 / envs.sim.freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 LQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 State Vector Representation\n",
    "\n",
    "We need to reformat the dict obervation returned by the simulation into a numpy array required by our controller:\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "x & \\dot{x} & y & \\dot{y} & z & \\dot{z} & \\phi & \\theta & \\psi & \\dot{\\phi} & \\dot{\\theta} & \\dot{\\psi}\n",
    "\\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "   - $x$: Position in the x-direction.\n",
    "   - $\\dot{x}$: Velocity in the x-direction (x\\_dot).\n",
    "   - $y$: Position in the y-direction.\n",
    "   - $\\dot{y}$: Velocity in the y-direction (y\\_dot).\n",
    "   - $z$: Position in the z-direction.\n",
    "   - $\\dot{z}$: Velocity in the z-direction (z\\_dot).\n",
    "   - $\\phi$: Roll angle.\n",
    "   - $\\theta$: Pitch angle.\n",
    "   - $\\psi$: Yaw angle.\n",
    "   - $\\dot{\\phi}$: Roll angular velocity (phi\\_dot).\n",
    "   - $\\dot{\\theta}$: Pitch angular velocity (theta\\_dot).\n",
    "   - $\\dot{\\psi}$: Yaw angular velocity (psi\\_dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_state(obs: dict[str, NDArray]) -> NDArray:\n",
    "    \"\"\"Convert the observation from the obs dictionary to the state vector.\"\"\"\n",
    "    # Extract position\n",
    "    pos = obs[\"pos\"].squeeze()  # shape: (3,)\n",
    "    \n",
    "    # Extract linear velocity\n",
    "    vel = obs[\"vel\"].squeeze()  # shape: (3,)\n",
    "    \n",
    "    # Extract orientation as quaternion and convert to Euler angles\n",
    "    quat = obs[\"quat\"].squeeze()  # shape: (4,)\n",
    "    euler = R.from_quat(quat).as_euler(\"xyz\")  # shape: (3,), Euler angles\n",
    "    # euler = euler[::-1] # [roll, pitch, yaw]\n",
    "    # Extract angular velocity\n",
    "    rpy_rates = obs[\"rpy_rates\"].squeeze()  # shape: (3,)\n",
    "    \n",
    "    # Concatenate into a single state vector\n",
    "    state = np.array([pos[0],vel[0],pos[1],vel[1],pos[2],vel[2],*euler, *rpy_rates])\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Linearization of system dynamik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve already created **symbolic representations** of the drone’s dynamics, **observation**, and **cost functions** using `CasADi`. For more details, please refer to `crazyflow/sim/symbolic.py`. The dynamics of the symbolic model is equivalent to the dynamics of the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `symbolic_model.df_func` function to compute the linearized system matrices _A_ and _B_ at the equilibrium states $x_{op}$ and $u_{op}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1 / envs.sim.freq\n",
    "symbolic_model = symbolic(MASS, J, dt)\n",
    "\n",
    "nx = 12 # dimension of state vector\n",
    "nu = 4 # dimension of input vector\n",
    "\n",
    "# Operating point\n",
    "x_op = np.array([0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0])  # State equilibrium \n",
    "u_op = np.ones(4, dtype=np.float32) * 0.25 * MASS * GRAVITY  # Control equilibrium\n",
    "\n",
    "df = symbolic_model.df_func(x_op, u_op)\n",
    "A, B = df[0].toarray(), df[1].toarray()\n",
    "\n",
    "print(\"A shape:\", A.shape)  # Should be (12, 12)\n",
    "print(\"B shape:\", B.shape)  # Should be (12, 4)\n",
    "# print(\"A :\\n\", A)\n",
    "# print(\"B :\\n\", B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "\n",
    "The ILQC controller requires **discretized, linearized system dynamics**:\n",
    "($A_k$, $B_k$)\n",
    "\n",
    "In contrast, the quadrotor model supplies **continuous, linearized system dynamics**:\n",
    "($A_{\\text{lin}}$, $B_{\\text{lin}}$)\n",
    "\n",
    "We need to define a function to discretize the linearized system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_linear_system(A: NDArray, B: NDArray, dt: float, exact: bool = False) -> tuple[NDArray, NDArray]:\n",
    "    \"\"\"Discretization of a linear system.\n",
    "\n",
    "    dx/dt = A x + B u --> xd[k+1] = Ad xd[k] + Bd ud[k] where xd[k] = x(k*dt)\n",
    "\n",
    "    Args:\n",
    "        A: System transition matrix.\n",
    "        B: Input matrix.\n",
    "        dt: Step time interval.\n",
    "        exact: Flag to use exact discretization.\n",
    "\n",
    "    Returns:\n",
    "        The discrete linear state matrix Ad and the discrete linear input matrix Bd.\n",
    "    \"\"\"\n",
    "    state_dim, input_dim = A.shape[1], B.shape[1]\n",
    "\n",
    "    if exact:\n",
    "        M = np.zeros((state_dim + input_dim, state_dim + input_dim))\n",
    "        M[:state_dim, :state_dim] = A\n",
    "        M[:state_dim, state_dim:] = B\n",
    "\n",
    "        Md = scipy.linalg.expm(M * dt)\n",
    "        Ad = Md[:state_dim, :state_dim]\n",
    "        Bd = Md[:state_dim, state_dim:]\n",
    "    else:\n",
    "        Identity = np.eye(state_dim)\n",
    "        Ad = Identity + A * dt\n",
    "        Bd = B * dt\n",
    "\n",
    "    return Ad, Bd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Quadratic cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_diag = [\n",
    "    5, 3,  # penalize position and velocity\n",
    "    5, 3,\n",
    "    5, 3,   \n",
    "    0.1, 0.1, 2,  # penalize orientation  \n",
    "    1, 1, 1  # penalize angular velocity\n",
    "]\n",
    "r_diag = [1, 1, 1, 1]\n",
    "Q_lqr = np.diag(q_diag)  # State cost\n",
    "R_lqr = np.diag(r_diag)  # Control cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 LQR controller gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ad, Bd = discretize_linear_system(A, B, dt)#, exact=True)\n",
    "\n",
    "P = scipy.linalg.solve_discrete_are(Ad, Bd, Q_lqr, R_lqr)\n",
    "\n",
    "btp = np.dot(Bd.T, P)\n",
    "\n",
    "gain_lqr = np.dot(np.linalg.inv(R_lqr + np.dot(btp, Bd)),\n",
    "                np.dot(btp, Ad))\n",
    "\n",
    "# We can also comment out the above two lines of code \n",
    "# and use the following line instead to compute for the continuous-time case\n",
    "\n",
    "# P = scipy.linalg.solve_continuous_are(A, B, Q_lqr, R_lqr)\n",
    "\n",
    "# gain_lqr = np.dot(np.linalg.inv(R_lqr), np.dot(B.T, P))\n",
    "\n",
    "# print(\"A (discretized):\\n\", Ad)\n",
    "# print(\"B (discretized):\\n\", Bd)\n",
    "print(\"gain:\\n\", gain_lqr)\n",
    "print(\"shape of gain:\", gain_lqr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Regulate the drone to the target state\n",
    "\n",
    "Note: you can use the `Tab`-key to switch between world and body camera in mujoco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "obs, info = envs.reset()#seed=SEED)\n",
    "state = obs_to_state(obs)\n",
    "# print(obs)\n",
    "# Step through the environment\n",
    "x_log = [state[0]]\n",
    "y_log = [state[2]]\n",
    "z_log = [state[4]]\n",
    "thrust_log = []\n",
    "fps = 60\n",
    "\n",
    "for i in range(2500):\n",
    "    \n",
    "    goal = np.array([0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0]) # set goal state\n",
    "\n",
    "    control_input = -gain_lqr @ (state - goal) + u_op\n",
    "    control_input = np.clip(control_input, MIN_THRUST, MAX_THRUST) \n",
    "    action = control_input.reshape(1,4).astype(np.float32)\n",
    "    # print(action)\n",
    "    thrust_log.append(action.flatten())\n",
    "    obs, reward, terminated, truncated, info = envs.step(action)\n",
    "\n",
    "    state = obs_to_state(obs)\n",
    "    # print('state:',state)\n",
    "    x_log.append(state[0])\n",
    "    y_log.append(state[2])\n",
    "    z_log.append(state[4])\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(\"Episode ended at step:\", i)\n",
    "        break\n",
    "    \n",
    "    if (i * fps) % envs.sim.freq < fps:\n",
    "        envs.render()\n",
    "        time.sleep(1 / fps)\n",
    "    \n",
    "envs.sim.close()\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.6 Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time array based on fixed step interval\n",
    "time_log = np.arange(len(x_log)) * dt\n",
    "\n",
    "# Plot theta and control input vs. time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_log, x_log, label=\"x\", color=\"blue\")\n",
    "plt.plot(time_log, y_log, label=\"y\", color=\"green\")\n",
    "plt.plot(time_log, z_log, label=\"z\", color=\"red\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"position\")\n",
    "plt.title(\"position vs Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drone was stabilized at the desired position: (0, 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of control input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrust_values = np.array(thrust_log)  # shape: (steps, 4)\n",
    "\n",
    "time_log = np.arange(thrust_values.shape[0]) * dt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_log, thrust_values[:, 0], label=\"Motor 1\", color=\"blue\")\n",
    "plt.plot(time_log, thrust_values[:, 1], label=\"Motor 2\", color=\"orange\")\n",
    "plt.plot(time_log, thrust_values[:, 2], label=\"Motor 3\", color=\"green\")\n",
    "plt.plot(time_log, thrust_values[:, 3], label=\"Motor 4\", color=\"red\")\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Thrust (N)')\n",
    "plt.title('Thrust over Time for Each Motor (lqr)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the begining, the thrusts reached the maximum thrust of motor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ILQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define desired point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d = np.array([0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0])  # State equilibrium \n",
    "u_d = np.ones(4, dtype=np.float32) * 0.25 * MASS * GRAVITY  # Control equilibrium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Initializaion of control policy\n",
    "\n",
    "Using LQR gain to initialize our control policy.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_k(x_k) & = \\bar{u}_k + \\delta u_k^* \\\\\n",
    "           & = \\bar{u}_k - gain * \\delta x_k \\\\\n",
    "           & = \\bar{u}_k - gain * (x_k - \\bar{x}_k) \\\\\n",
    "           & = \\underbrace{\\bar{u}_k + gain * \\bar{x}_k}_{\\text{feedforward } u_{k,\\text{ff}}} \n",
    "            \\underbrace{ - gain * x_k}_{\\text{feedback}} \\\\\n",
    "           & = u_{k,\\text{ff}} + gain_{fb} * x_k.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ff = u_op + gain_lqr.dot(x_op)\n",
    "gains_fb = - gain_lqr\n",
    "\n",
    "input_ff = np.tile(input_ff.reshape(4, 1), (1, 2500))\n",
    "gains_fb = np.tile(gains_fb.reshape(1, nu, nx), (2500, 1, 1))\n",
    "\n",
    "print('Shape of input_ff:',input_ff.shape)\n",
    "print('Shape of gains_fb:',gains_fb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Define cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_diag = [\n",
    "    5, 3,\n",
    "    5, 3, \n",
    "    5, 3,   \n",
    "    0.1, 0.1, 2,      \n",
    "    1, 1, 1  \n",
    "]\n",
    "r_diag = [1, 1, 1, 1]\n",
    "Q_ilqr = np.diag(q_diag)  # State cost weight\n",
    "R_ilqr = np.diag(r_diag)  # Input cost weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_stack = 0\n",
    "iter = 0\n",
    "max_iteration = 15\n",
    "duff = 0\n",
    "norm_threshold = 0.01\n",
    "cost = []  #\n",
    "\n",
    "while iter <= max_iteration and (np.linalg.norm(np.squeeze(duff)) > norm_threshold or iter == 0):\n",
    "    \n",
    "    input_pre = input_stack\n",
    "\n",
    "    # Forward / \"rollout\" of the current policy\n",
    "    obs, info = envs.reset() #seed=SEED)\n",
    "    state = obs_to_state(obs) # (12,)\n",
    "\n",
    "    for step in range(2500):\n",
    "\n",
    "        # Calculate control input.\n",
    "        control_input = input_ff[:, step] + gains_fb[step].dot(state) \n",
    "        \n",
    "        # Clip the control input to the specified range\n",
    "        control_input = np.clip(control_input, 0.028161688, 0.14834145) \n",
    "        \n",
    "        # Convert to np.ndarray\n",
    "        action = np.array([control_input], dtype=np.float32) # (1, 4)\n",
    "\n",
    "        # Save rollout data.\n",
    "        if step == 0:\n",
    "            # Initialize state and input stack.\n",
    "            state_stack = state\n",
    "            input_stack = action\n",
    "        else:\n",
    "            # Save state and input.\n",
    "            state_stack = np.vstack((state_stack, state)) # (N, 12)\n",
    "            input_stack = np.vstack((input_stack, action)) # (N, 4)\n",
    "\n",
    "        # Step forward.\n",
    "        obs, reward, terminated, truncated, _ = envs.step(action)\n",
    "        state = obs_to_state(obs) # (12,)\n",
    "        \n",
    "    envs.close()\n",
    "    \n",
    "    # TODO: Compute cost to see if it could converge\n",
    "    # cost_curr = 0\n",
    "    # for i in range(state_stack.shape[0]):\n",
    "\n",
    "    # Initialize backward pass.\n",
    "    state_k = state_stack[-1].reshape(-1, 1) # (12, 1)\n",
    "    input_k = u_op.reshape(-1, 1) # (4, 1)\n",
    "\n",
    "    loss_k = symbolic_model.loss(\n",
    "                        x=state_k,\n",
    "                        u=input_k,\n",
    "                        Xr=x_d,\n",
    "                        Ur=u_d,\n",
    "                        Q=Q_ilqr,\n",
    "                        R=R_ilqr)\n",
    "    s = loss_k['l'].toarray()\n",
    "    Sv = loss_k['l_x'].toarray().transpose()\n",
    "    Sm = loss_k['l_xx'].toarray().transpose()\n",
    "\n",
    "    # \"Backward pass\": Calculate the coefficients (s,Sv,Sm) for the value\n",
    "    # functions at earlier times by proceeding backwards in time\n",
    "    # (DP-approach)\n",
    "\n",
    "    for k in reversed(range(2500)):\n",
    "\n",
    "        # Get current operating point.\n",
    "        state_k = state_stack[k] #.reshape(-1, 1) # (12,)\n",
    "        input_k = input_stack[k] #.reshape(-1, 1) # (4,)\n",
    "\n",
    "        # Linearized dynamics about (x_k, u_k).\n",
    "        df_k = symbolic_model.df_func(state_k, input_k)\n",
    "        Ac_k, Bc_k = df_k[0].toarray(), df_k[1].toarray()\n",
    "        Ad_k, Bd_k = discretize_linear_system(Ac_k, Bc_k, dt)\n",
    "\n",
    "        # Jacobian and Hessian of the loss w.r.t. state and input.\n",
    "        loss_k = symbolic_model.loss(x=state_k,\n",
    "                                    u=input_k,\n",
    "                                    Xr=x_d,\n",
    "                                    Ur=u_d,\n",
    "                                    Q=Q_ilqr,\n",
    "                                    R=R_ilqr)\n",
    "\n",
    "        # Quadratic approximation of cost.\n",
    "        q = loss_k['l'].toarray()  # l\n",
    "        Qv = loss_k['l_x'].toarray().transpose()  # dl/dx\n",
    "        Qm = loss_k['l_xx'].toarray().transpose()  # ddl/dxdx\n",
    "        Rv = loss_k['l_u'].toarray().transpose()  # dl/du\n",
    "        Rm = loss_k['l_uu'].toarray().transpose()  # ddl/dudu\n",
    "        Pm = loss_k['l_xu'].toarray().transpose()  # ddl/dudx\n",
    "\n",
    "        # Control dependent terms of cost function.\n",
    "        g = Rv + Bd_k.transpose().dot(Sv)\n",
    "        G = Pm + Bd_k.transpose().dot(Sm.dot(Ad_k))\n",
    "        H = Rm + Bd_k.transpose().dot(Sm.dot(Bd_k))\n",
    "\n",
    "        H = (H + H.transpose()) / 2\n",
    "        # Trick to make sure H is well-conditioned for inversion\n",
    "        # if not (np.isinf(np.sum(H)) or np.isnan(np.sum(H))):\n",
    "        #     H = (H + H.transpose()) / 2\n",
    "        #     H_eval, H_evec = np.linalg.eig(H)\n",
    "        #     H_eval[H_eval < 0] = 0.0\n",
    "        #     H_eval += 1e-6 #lamb\n",
    "\n",
    "        # X = T^-1 * (L * + 1e-6) * T\n",
    "\n",
    "        #     H_inv = np.dot(H_evec, np.dot(np.diag(1.0 / H_eval), H_evec.T))\n",
    "\n",
    "        # Update controller gains.\n",
    "        assert not np.isinf(H).any(), f\"H contains Inf values at time step {k}\"\n",
    "        assert not np.isnan(H).any(), f\"H is NaN at timestep {k} at {iter}\"\n",
    "        H_inv = np.linalg.inv(H)\n",
    "        assert not np.isnan(H_inv).any(), f\"H_inv is NaN at timestep {k}\"\n",
    "        assert not np.isinf(H_inv).any(), f\"H_inv contains Inf values at time step {k}\"\n",
    "\n",
    "        duff = -H_inv.dot(g)\n",
    "        K = -H_inv.dot(G)\n",
    "\n",
    "        # Update control input.\n",
    "        input_ff_k = input_k + duff[:, 0] - K.dot(state_k)\n",
    "        input_ff[:, k] = input_ff_k\n",
    "        gains_fb[k] = K\n",
    "\n",
    "        # Update s variables for time step k.\n",
    "        Sm = Qm + Ad_k.transpose().dot(Sm.dot(Ad_k)) + \\\n",
    "            K.transpose().dot(H.dot(K)) + \\\n",
    "            K.transpose().dot(G) + G.transpose().dot(K)\n",
    "        Sv = Qv + Ad_k.transpose().dot(Sv) + \\\n",
    "            K.transpose().dot(H.dot(duff)) + K.transpose().dot(g) + \\\n",
    "            G.transpose().dot(duff)\n",
    "        s = q + s + 0.5 * duff.transpose().dot(H.dot(duff)) + \\\n",
    "            duff.transpose().dot(g)\n",
    "    \n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Simulate and plot results\n",
    "obs, _ = envs.reset()#seed=42)\n",
    "state = obs_to_state(obs)\n",
    "\n",
    "x_log_ilqr = [state[0]]\n",
    "y_log_ilqr = [state[2]]\n",
    "z_log_ilqr = [state[4]]\n",
    "thrust_log_ilqr = []\n",
    "\n",
    "# Simulation loop\n",
    "for i in range(2500):  # Simulate for 2500 steps\n",
    "    # envs.render()\n",
    "\n",
    "    # Compute control action (force) using the iLQR gain\n",
    "    control_input = input_ff[:, i] + gains_fb[i].dot(state) # gains_fb[:, i].dot(state) + input_ff[i]\n",
    "\n",
    "    # Clip the control iptput to the specified range\n",
    "    control_input = np.clip(control_input, 0.028161688, 0.14834145)\n",
    "    \n",
    "    # Convert to np.ndarray\n",
    "    action = np.array([control_input], dtype=np.float32)           \n",
    "\n",
    "    # Take a step in the environment with the computed action\n",
    "    obs, reward, terminated, truncated, _ = envs.step(action)\n",
    "    state = obs_to_state(obs)\n",
    "\n",
    "    # Log data\n",
    "    x_log_ilqr.append(state[0])\n",
    "    y_log_ilqr.append(state[2])\n",
    "    z_log_ilqr.append(state[4])\n",
    "    thrust_log_ilqr.append(action.flatten())\n",
    "\n",
    "    # Check if the episode is terminated\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode ended at step:\", i)\n",
    "        break\n",
    "    envs.render()\n",
    "# Close the environment\n",
    "envs.sim.close()\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time array based on fixed step interval\n",
    "time_log = np.arange(len(x_log_ilqr)) * dt\n",
    "\n",
    "# Plot theta and control input vs. time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_log, x_log_ilqr, label=\"x(iLQR)\", color=\"blue\")\n",
    "plt.plot(time_log, y_log_ilqr, label=\"y(iLQR)\", color=\"green\")\n",
    "plt.plot(time_log, z_log_ilqr, label=\"z(iLQR)\", color=\"red\")\n",
    "# plt.plot(time_log, x_log, label=\"x(LQR)\", color=\"blue\", linestyle=\"--\")\n",
    "# plt.plot(time_log, y_log, label=\"y(LQR)\", color=\"green\", linestyle=\"--\")\n",
    "# plt.plot(time_log, z_log, label=\"z(LQR)\", color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"position\")\n",
    "plt.title(\"position vs Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrust_values_ilqr = np.array(thrust_log_ilqr)  # shape: (steps, 4)\n",
    "\n",
    "time_log = np.arange(thrust_values.shape[0]) * dt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_log, thrust_values_ilqr[:, 0], label=\"Motor 1(iLQR)\", color=\"blue\")\n",
    "plt.plot(time_log, thrust_values_ilqr[:, 1], label=\"Motor 2(iLQR)\", color=\"orange\")\n",
    "plt.plot(time_log, thrust_values_ilqr[:, 2], label=\"Motor 3(iLQR)\", color=\"green\")\n",
    "plt.plot(time_log, thrust_values_ilqr[:, 3], label=\"Motor 4(iLQR)\", color=\"red\")\n",
    "\n",
    "# plt.plot(time_log, thrust_values[:, 0], label=\"Motor 1(LQR)\", color=\"blue\", linestyle=\"--\" )\n",
    "# plt.plot(time_log, thrust_values[:, 1], label=\"Motor 2(LQR)\", color=\"orange\", linestyle=\"--\")\n",
    "# plt.plot(time_log, thrust_values[:, 2], label=\"Motor 3(LQR)\", color=\"green\", linestyle=\"--\")\n",
    "# plt.plot(time_log, thrust_values[:, 3], label=\"Motor 4(LQR)\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Thrust (N)')\n",
    "plt.title('Thrust over Time for Each Motor (lqr)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
