{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: LQR & ILQR\n",
    "\n",
    "In this exercise you will learn how to develop your own LQR and ILQR algorithms. For this task we need to use CasADi, a library for symbolic calculation for python. This library composes functionalities not only symbolic calculation, but also automatic differentiation, numerical integration, linear and nonlinear programming solvers. We find its various use cases in control engineering such as control for quadrotors and automotives. \n",
    "\n",
    "For tutiorials and usages please refer to the [CasADi documentation](https://web.casadi.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import casadi as cs\n",
    "from ml_collections import config_dict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output \n",
    "\n",
    "from crazyflow.control.controller import Control, Controller\n",
    "from crazyflow.sim.physics import Physics\n",
    "\n",
    "from crazyflow.sim.symbolic import symbolic\n",
    "from crazyflow.constants import MASS, GRAVITY, J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config for simulation\n",
    "sim_config = config_dict.ConfigDict()\n",
    "sim_config.device = \"cpu\"\n",
    "sim_config.physics = Physics.default\n",
    "sim_config.control = Control.thrust\n",
    "sim_config.controller = Controller.default\n",
    "sim_config.control_freq = 500\n",
    "sim_config.n_drones = 1\n",
    "sim_config.n_worlds = 1 #20\n",
    "\n",
    "envs = gymnasium.make_vec(\n",
    "    \"DroneReachPos-v0\",\n",
    "    return_datatype=\"numpy\",\n",
    "    num_envs=sim_config.n_worlds,\n",
    "    **sim_config,\n",
    ")\n",
    "\n",
    "print('observation space: \\n', envs.observation_space)\n",
    "print('action space: \\n', envs.action_space)\n",
    "print('time step:', envs.sim.dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve already created **symbolic representations** of the drone’s dynamics, **observation**, and **cost functions** using `CasADi`. For more details, please refer to `crazyflow/sim/symbolic.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the symbolic_model.df_func function to compute the linearized system matrices _A_ and _B_ at the equilibrium states $x_{op}$ and $u_{op}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = envs.sim.dt\n",
    "symbolic_model = symbolic(MASS, J, dt)\n",
    "\n",
    "nx = 12 # dimention of state vector\n",
    "nu = 4 # dimention of input vector\n",
    "\n",
    "# Operating point\n",
    "x_op = np.array([0, 0, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0])  # State equilibrium \n",
    "u_op = np.ones(4, dtype=np.float32) * 0.06 #0.25 * MASS * GRAVITY  #0.06134# Control equilibrium\n",
    "# u_op = np.zeros((1, 4), dtype=np.float32) # input for attitude control\n",
    "# u_op[:, 0] = MASS * GRAVITY \n",
    "\n",
    "df = symbolic_model.df_func(x_op, u_op)\n",
    "A, B = df[0].toarray(), df[1].toarray()\n",
    "\n",
    "print(\"A shape:\", A.shape)  # Should be (12, 12)\n",
    "print(\"B shape:\", B.shape)  # Should be (12, 4)\n",
    "print(\"A :\\n\", A)\n",
    "print(\"B :\\n\", B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_linear_system(A, B, dt, exact=False):\n",
    "    '''Discretization of a linear system\n",
    "\n",
    "    dx/dt = A x + B u\n",
    "    --> xd[k+1] = Ad xd[k] + Bd ud[k] where xd[k] = x(k*dt)\n",
    "\n",
    "    Args:\n",
    "        A (ndarray): System transition matrix.\n",
    "        B (ndarray): Input matrix.\n",
    "        dt (scalar): Step time interval.\n",
    "        exact (bool): If to use exact discretization.\n",
    "\n",
    "    Returns:\n",
    "        Ad (ndarray): The discrete linear state matrix A.\n",
    "        Bd (ndarray): The discrete linear input matrix B.\n",
    "    '''\n",
    "\n",
    "    state_dim, input_dim = A.shape[1], B.shape[1]\n",
    "\n",
    "    if exact:\n",
    "        M = np.zeros((state_dim + input_dim, state_dim + input_dim))\n",
    "        M[:state_dim, :state_dim] = A\n",
    "        M[:state_dim, state_dim:] = B\n",
    "\n",
    "        Md = scipy.linalg.expm(M * dt)\n",
    "        Ad = Md[:state_dim, :state_dim]\n",
    "        Bd = Md[:state_dim, state_dim:]\n",
    "    else:\n",
    "        Identity = np.eye(state_dim)\n",
    "        Ad = Identity + A * dt\n",
    "        Bd = B * dt\n",
    "\n",
    "    return Ad, Bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Implement LQR\n",
    "q_diag = [\n",
    "    0.1, 0.1, 0.1,       # penalize positions\n",
    "    0.1, 0.1, 0.1,   # penalize linear velocities\n",
    "    0.1, 0.1, 2,       # penalize orientations\n",
    "    1, 1, 1        # penalize angular velocities\n",
    "]\n",
    "r_diag = [0.01, 0.01, 0.01, 0.01]\n",
    "Q_lqr = np.diag(q_diag)  # State cost\n",
    "R_lqr = np.diag(r_diag)  # Control cost\n",
    "\n",
    "# Ad, Bd = discretize_linear_system(A, B, dt)\n",
    "# P = scipy.linalg.solve_discrete_are(Ad, Bd, Q_lqr, R_lqr)\n",
    "# btp = np.dot(Bd.T, P)\n",
    "# gain_lqr = np.dot(np.linalg.inv(R_lqr + np.dot(btp, Bd)),\n",
    "#                 np.dot(btp, Ad))\n",
    "\n",
    "# We can also comment out the above two lines of code \n",
    "# and use the following line instead to compute for the continuous-time case\n",
    "\n",
    "P = scipy.linalg.solve_continuous_are(A, B, Q_lqr, R_lqr)\n",
    "\n",
    "gain_lqr = np.dot(np.linalg.inv(R_lqr), np.dot(B.T, P))\n",
    "\n",
    "# print(\"A (discretized):\\n\", Ad)\n",
    "# print(\"B (discretized):\\n\", Bd)\n",
    "print(\"gain:\\n\", gain_lqr)\n",
    "print(\"shape of gain:\", gain_lqr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def obs_to_state(obs):\n",
    "    # Extract position\n",
    "    pos = obs[\"pos\"].squeeze()  # shape: (3,)\n",
    "    \n",
    "    # Extract linear velocity\n",
    "    vel = obs[\"vel\"].squeeze()  # shape: (3,)\n",
    "    \n",
    "    # Extract orientation as quaternion and convert to Euler angles\n",
    "    quat = obs[\"quat\"].squeeze()  # shape: (4,)\n",
    "    euler = R.from_quat(quat).as_euler(\"zyx\")  # shape: (3,), Euler angles: [yaw, pitch, roll]\n",
    "    euler = euler[::-1] # [roll, pitch, yaw]\n",
    "    # Extract angular velocity\n",
    "    ang_vel = obs[\"rpy_rates\"].squeeze()  # shape: (3,)\n",
    "    \n",
    "    # Concatenate into a single state vector\n",
    "    state = np.concatenate([pos, vel, euler, ang_vel])  # shape: (12,)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action for going up (in attitude control). NOTE actions are rescaled in the environment\n",
    "# action = np.zeros((sim_config.n_worlds * sim_config.n_drones, 4), dtype=np.float32)\n",
    "# action[..., 0] = -0.2\n",
    "SEED = 42\n",
    "\n",
    "obs, info = envs.reset_all()#seed=SEED)\n",
    "print(obs)\n",
    "# Step through the environment\n",
    "x_log = []\n",
    "y_log = []\n",
    "z_log = []\n",
    "thrust_log = []\n",
    "\n",
    "for i in range(3000):\n",
    "    state = obs_to_state(obs)\n",
    "    print('state:',state)\n",
    "    x_log.append(state[0])\n",
    "    y_log.append(state[1])\n",
    "    z_log.append(state[2])\n",
    "    # print(state)\n",
    "    control_input = -gain_lqr @ (state - x_op) + u_op\n",
    "    control_input = np.clip(control_input, 0.028161688, 0.14834145) \n",
    "    action = control_input.reshape(1,4).astype(np.float32)\n",
    "    # print(action)\n",
    "    thrust_log.append(action.flatten())\n",
    "    obs, reward, terminated, truncated, info = envs.step(action)\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode ended at step:\", i)\n",
    "        break\n",
    "    envs.render()\n",
    "    \n",
    "envs.sim.close()\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(envs.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time array based on fixed step interval\n",
    "time_log = np.arange(len(x_log)) * dt\n",
    "\n",
    "# Plot theta and control input vs. time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_log, x_log, label=\"x\", color=\"blue\")\n",
    "plt.plot(time_log, y_log, label=\"y\", color=\"green\")\n",
    "plt.plot(time_log, z_log, label=\"z\", color=\"red\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"position\")\n",
    "plt.title(\"position vs Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrust_values = np.array(thrust_log)  # shape: (steps, 4)\n",
    "\n",
    "time_log = np.arange(thrust_values.shape[0]) * dt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_log, thrust_values[:, 0], label=\"Motor 1\", color=\"blue\")\n",
    "plt.plot(time_log, thrust_values[:, 1], label=\"Motor 2\", color=\"orange\")\n",
    "plt.plot(time_log, thrust_values[:, 2], label=\"Motor 3\", color=\"green\")\n",
    "plt.plot(time_log, thrust_values[:, 3], label=\"Motor 4\", color=\"red\")\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Thrust (N)')\n",
    "plt.title('Thrust over Time for Each Motor (lqr)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thrust_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ILQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: iLQR Implementation\n",
    "input_ff = u_op + gain_lqr.dot(x_op)\n",
    "gains_fb = - gain_lqr\n",
    "\n",
    "# input_ff = np.tile(input_ff, (200,1))\n",
    "input_ff = np.tile(input_ff.reshape(4, 1), (1, 200))\n",
    "gains_fb = np.tile(gains_fb.reshape(1, nu, nx), (200, 1, 1))\n",
    "\n",
    "print(input_ff.shape)\n",
    "print(gains_fb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_diag = [\n",
    "    1, 1, 1,       # penalize positions\n",
    "    3, 3, 3,   # penalize linear velocities\n",
    "    0.1, 0.1, 2,       # penalize orientations\n",
    "    1, 1, 1        # penalize angular velocities\n",
    "]\n",
    "r_diag = [1, 1, 1, 1]\n",
    "Q_ilqr = np.diag(q_diag)  # State cost\n",
    "R_ilqr = np.diag(r_diag)  # Control cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_stack = 0\n",
    "\n",
    "for iter in range(15):\n",
    "    \n",
    "    input_pre = input_stack\n",
    "\n",
    "    # Forward\n",
    "    obs, info = envs.reset_all(seed=SEED)\n",
    "    for step in range(200):\n",
    "        \n",
    "        state = obs_to_state(obs)\n",
    "        # Calculate control input.\n",
    "        control_input = gains_fb[step].dot(state) + input_ff[:, step]\n",
    "        # Clip the control input to the specified range\n",
    "        control_input = np.clip(control_input, 0.028161688, 0.14834145) \n",
    "        \n",
    "        # Convert to np.ndarray\n",
    "        action = np.array([control_input], dtype=np.float32) \n",
    "        # Save rollout data.\n",
    "        if step == 0:\n",
    "            # Initialize state and input stack.\n",
    "            state_stack = state\n",
    "            input_stack = action\n",
    "        else:\n",
    "            # Save state and input.\n",
    "            # print(\"state_stack size:\", state_stack.shape)\n",
    "            # print(\"state size:\", state.shape)\n",
    "            # print(\"input_stack size:\", input_stack.shape)\n",
    "            # print(\"input size:\", action.shape)\n",
    "            state_stack = np.vstack((state_stack, state))\n",
    "            input_stack = np.vstack((input_stack, action))\n",
    "\n",
    "        # Step forward.\n",
    "        obs, reward, terminated, truncated, _ = envs.step(action)\n",
    "    envs.close()\n",
    "\n",
    "    # Initialize backward pass.\n",
    "    # state_k = cs.MX(state_stack[-1])\n",
    "    state_k = state_stack[-1].reshape(-1, 1)\n",
    "    input_k = u_op.reshape(-1, 1)\n",
    "\n",
    "    # print(\"state_k type:\", type(state_k))\n",
    "    # print(state_stack.shape)\n",
    "    # print(state_k.shape)\n",
    "    # print(\"input_k type:\", type(input_k))\n",
    "    # print(input_k.shape)\n",
    "    # print(\"x_op type:\", type(x_op))\n",
    "    # print(\"u_op type:\", type(u_op))\n",
    "    # print(type(Q))\n",
    "    # print(type(R))\n",
    "    # print(R)\n",
    "\n",
    "    loss_k = symbolic_model.loss(\n",
    "                        x=state_k,\n",
    "                        u=input_k,\n",
    "                        Xr=x_op,\n",
    "                        Ur=u_op,\n",
    "                        Q=Q_ilqr,\n",
    "                        R=R_ilqr)\n",
    "    s = loss_k['l'].toarray()\n",
    "    Sv = loss_k['l_x'].toarray().transpose()\n",
    "    Sm = loss_k['l_xx'].toarray().transpose()\n",
    "\n",
    "    # Backward pass.\n",
    "    for k in reversed(range(200)):\n",
    "        # Get current operating point.\n",
    "        state_k = state_stack[k] #.reshape(-1, 1)\n",
    "        input_k = input_stack[k] #.reshape(-1, 1)\n",
    "\n",
    "        # Linearized dynamics about (x_k, u_k).\n",
    "        df_k = symbolic_model.df_func(state_k, input_k)\n",
    "        Ac_k, Bc_k = df_k[0].toarray(), df_k[1].toarray()\n",
    "        Ad_k, Bd_k = discretize_linear_system(Ac_k, Bc_k, dt)\n",
    "\n",
    "        # Jacobian and Hessian of the loss w.r.t. state and input.\n",
    "        loss_k = symbolic_model.loss(x=state_k,\n",
    "                                    u=input_k,\n",
    "                                    Xr=x_op,\n",
    "                                    Ur=u_op,\n",
    "                                    Q=Q_ilqr,\n",
    "                                    R=R_ilqr)\n",
    "\n",
    "        # Quadratic approximation of cost.\n",
    "        q = loss_k['l'].toarray()  # l\n",
    "        Qv = loss_k['l_x'].toarray().transpose()  # dl/dx\n",
    "        Qm = loss_k['l_xx'].toarray().transpose()  # ddl/dxdx\n",
    "        Rv = loss_k['l_u'].toarray().transpose()  # dl/du\n",
    "        Rm = loss_k['l_uu'].toarray().transpose()  # ddl/dudu\n",
    "        Pm = loss_k['l_xu'].toarray().transpose()  # ddl/dudx\n",
    "\n",
    "        # Control dependent terms of cost function.\n",
    "        g = Rv + Bd_k.transpose().dot(Sv)\n",
    "        G = Pm + Bd_k.transpose().dot(Sm.dot(Ad_k))\n",
    "        H = Rm + Bd_k.transpose().dot(Sm.dot(Bd_k))\n",
    "        \n",
    "        # Trick to make sure H is well-conditioned for inversion\n",
    "        # if not (np.isinf(np.sum(H)) or np.isnan(np.sum(H))):\n",
    "        #     H = (H + H.transpose()) / 2\n",
    "        #     H_eval, H_evec = np.linalg.eig(H)\n",
    "        #     H_eval[H_eval < 0] = 0.0\n",
    "        #     H_eval += 1e-6 #lamb\n",
    "        #     H_inv = np.dot(H_evec, np.dot(np.diag(1.0 / H_eval), H_evec.T))\n",
    "\n",
    "        # Update controller gains.\n",
    "        H_inv = np.linalg.inv(H)\n",
    "        duff = -H_inv.dot(g)\n",
    "        K = -H_inv.dot(G)\n",
    "\n",
    "        # Update control input.\n",
    "        input_ff_k = input_k + duff[:, 0] - K.dot(state_k)\n",
    "        input_ff[:, k] = input_ff_k\n",
    "        gains_fb[k] = K\n",
    "\n",
    "        # Update s variables for time step k.\n",
    "        Sm = Qm + Ad_k.transpose().dot(Sm.dot(Ad_k)) + \\\n",
    "            K.transpose().dot(H.dot(K)) + \\\n",
    "            K.transpose().dot(G) + G.transpose().dot(K)\n",
    "        Sv = Qv + Ad_k.transpose().dot(Sv) + \\\n",
    "            K.transpose().dot(H.dot(duff)) + K.transpose().dot(g) + \\\n",
    "            G.transpose().dot(duff)\n",
    "        s = q + s + 0.5 * duff.transpose().dot(H.dot(duff)) + \\\n",
    "            duff.transpose().dot(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
